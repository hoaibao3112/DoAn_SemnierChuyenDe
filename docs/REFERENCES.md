# Tài liệu tham khảo

Dưới đây là các tài liệu, bài báo và tài nguyên tham khảo chính được sử dụng trong đồ án.

- Devlin, J., Chang, M.-W., Lee, K., & Toutanova, K. (2019). BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. *arXiv preprint arXiv:1810.04805*. https://arxiv.org/abs/1810.04805
- Nguyen, V.-A., et al. (2020). PhoBERT: Pre-trained language models for Vietnamese. *arXiv preprint arXiv:2003.00744*. https://arxiv.org/abs/2003.00744
- Conneau, A., et al. (2020). Unsupervised Cross-lingual Representation Learning at Scale. (XLM-R). *arXiv preprint arXiv:1911.02116*. https://arxiv.org/abs/1911.02116
- nlptown/bert-base-multilingual-uncased-sentiment (Hugging Face model card). https://huggingface.co/nlptown/bert-base-multilingual-uncased-sentiment
- Hugging Face — Transformers documentation. https://huggingface.co/docs/transformers/
- Streamlit documentation. https://docs.streamlit.io/
- Underthesea (thư viện NLP tiếng Việt). https://underthesea.readthedocs.io/
- Open Web (technical resources): Hugging Face Hub, model cards and caching behavior. https://huggingface.co/
- SQLite documentation. https://www.sqlite.org/docs.html
- Papers / resources on sentiment analysis and evaluation best practices (overview):
  - Liu, B. (2012). Sentiment Analysis and Opinion Mining. *Synthesis Lectures on Human Language Technologies*.

--
_Gợi ý:_ khi trích dẫn trong báo cáo chính thức, trình bày theo chuẩn trích dẫn của trường/giảng viên (APA/IEEE). Nếu bạn muốn, tôi sẽ sinh phần References theo định dạng APA hoặc IEEE.
